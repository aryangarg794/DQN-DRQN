{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- https://arxiv.org/pdf/1507.06527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers.frame_stack import LazyFrames\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    AtariWrapper,\n",
    "    FireResetEnv,\n",
    ")\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "from utils.replay import LazyFramesToNumpyWrapper, MetricTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame, gray=False):\n",
    "    if gray:\n",
    "        plt.imshow(frame, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_multiple_frames(frames):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(frames[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickeringAtari(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def observation(self, observation, prob=0.5):\n",
    "        if np.random.rand() < prob:\n",
    "            observation = np.zeros_like(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(game, render='rgb_array'):\n",
    "    env = gym.make(game, render_mode=render)\n",
    "    env = AtariWrapper(env, terminal_on_life_loss=False)\n",
    "    env = LazyFramesToNumpyWrapper(env)\n",
    "    env = FlickeringAtari(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = make_env('FrostbiteNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, _ = env_test.reset()\n",
    "print(observation.shape)\n",
    "display_frame(observation, gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Episode:\n",
    "    \n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.obs_prime = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add_step(\n",
    "        self,\n",
    "        observation,\n",
    "        action, \n",
    "        reward, \n",
    "        observation_prime, \n",
    "        terminated\n",
    "    ):\n",
    "        self.obs.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.obs_prime.append(observation_prime)\n",
    "        self.dones.append(terminated)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, slice):\n",
    "            ep = Episode()\n",
    "            ep.obs = self.obs[index]\n",
    "            ep.actions = self.actions[index]\n",
    "            ep.rewards = self.rewards[index]\n",
    "            ep.obs_prime = self.obs_prime[index]\n",
    "            ep.dones = self.dones[index]\n",
    "            return ep\n",
    "        elif isinstance(index, tuple):\n",
    "            arg = index[0]\n",
    "            pos = index[1]\n",
    "            if arg == 0: \n",
    "                return self.obs[pos]\n",
    "            elif arg == 1:\n",
    "                return self.actions[pos]\n",
    "            elif arg == 2:\n",
    "                return self.rewards[pos]\n",
    "            elif arg == 3:\n",
    "                return self.obs_prime[pos]\n",
    "            elif arg == 4:\n",
    "                return self.dones[pos]\n",
    "         \n",
    "        return (\n",
    "            self.obs[index],\n",
    "            self.actions[index],\n",
    "            self.rewards[index],\n",
    "            self.obs_prime[index],\n",
    "            self.dones[index]\n",
    "        )\n",
    "    \n",
    "    def pad(self, stop):\n",
    "        new_ep = Episode()\n",
    "        new_ep.obs = deepcopy(self.obs)\n",
    "        new_ep.actions = deepcopy(self.actions)\n",
    "        new_ep.rewards = deepcopy(self.rewards)\n",
    "        new_ep.obs_prime = deepcopy(self.obs_prime)\n",
    "        new_ep.dones = deepcopy(self.dones)\n",
    "        def pad_field(arr, stop, zeros):\n",
    "            for i in range(stop):\n",
    "                if i >= len(arr):\n",
    "                    arr.append(zeros)\n",
    "        \n",
    "        pad_field(new_ep.obs, stop, zeros=np.zeros_like(self.obs[0]))\n",
    "        pad_field(new_ep.actions, stop, zeros=np.int64(0))\n",
    "        pad_field(new_ep.rewards, stop, zeros=np.float32(0.0))\n",
    "        pad_field(new_ep.obs_prime, stop, zeros=np.zeros_like(self.obs_prime[0]))\n",
    "        pad_field(new_ep.dones, stop, zeros=True)\n",
    "        \n",
    "        return new_ep\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Episode of Length {len(self)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Episode of Length 316"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, _ = env_test.reset()\n",
    "done = False\n",
    "ep = Episode()\n",
    "while not done:\n",
    "    action = env_test.action_space.sample()\n",
    "    observation_prime, reward, terminated, truncated, _ = env_test.step(action) \n",
    "    ep.add_step(\n",
    "        observation.squeeze(),\n",
    "        action, \n",
    "        reward, \n",
    "        observation_prime.squeeze(),\n",
    "        terminated or truncated\n",
    "    )\n",
    "    \n",
    "    done = terminated or truncated\n",
    "    observation = observation_prime\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Episode of Length 316, Episode of Length 500)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ep = ep.pad(500)\n",
    "ep, padded_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBufferDRQN:\n",
    "    \n",
    "    def __init__(self, capacity, device='cuda'):\n",
    "        self._buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "    \n",
    "    def store(self, episode):\n",
    "        self._buffer.append(episode)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self._buffer, batch_size)\n",
    "        max_len = len(max(batch, key=len))\n",
    "        padded_batch = [ep.pad(max_len) for ep in batch]\n",
    "        \n",
    "        return (\n",
    "            self.extract_item(padded_batch, 0, (batch_size, max_len, 1, 84, 84), dtype=torch.float32),\n",
    "            self.extract_item(padded_batch, 1, (batch_size, max_len, 1), dtype=torch.int64),\n",
    "            self.extract_item(padded_batch, 2, (batch_size, max_len, 1), dtype=torch.float32),\n",
    "            self.extract_item(padded_batch, 3, (batch_size, max_len, 1, 84, 84), dtype=torch.float32),\n",
    "            self.extract_item(padded_batch, 4, (batch_size, max_len, 1), dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    def extract_item(self, batch, arg, shape, dtype):\n",
    "        batched_values = np.array([val[arg, :] for val in batch]).reshape(shape)\n",
    "        batched_values = torch.as_tensor(batched_values, dtype=dtype, device=self.device)\n",
    "        return batched_values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._buffer[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBufferDRQN(10)\n",
    "\n",
    "observation, _ = env_test.reset()\n",
    "for _ in range(10):    \n",
    "    observation, _ = env_test.reset()\n",
    "    done = False\n",
    "    ep = Episode()\n",
    "    while not done:\n",
    "        action = env_test.action_space.sample()\n",
    "        observation_prime, reward, terminated, truncated, _ = env_test.step(action) \n",
    "        ep.add_step(\n",
    "            observation.squeeze(),\n",
    "            action, \n",
    "            reward, \n",
    "            observation_prime.squeeze(),\n",
    "            terminated or truncated\n",
    "        )\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        observation = observation_prime\n",
    "        \n",
    "    buffer.store(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs, test_act, test_rew, test_prime, test_dones = buffer.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 337, 1, 84, 84]),\n",
       " torch.Size([2, 337, 1]),\n",
       " torch.Size([2, 337, 1]),\n",
       " torch.Size([2, 337, 1, 84, 84]),\n",
       " torch.Size([2, 337, 1]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_obs.shape, test_act.shape, test_rew.shape, test_prime.shape, test_dones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_buffer(preload, capacity, game, prob=0.5):\n",
    "    env = make_env(game)\n",
    "    buffer = ReplayBufferDRQN(capacity)\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    for _ in range(preload):    \n",
    "        observation, _ = env_test.reset()\n",
    "        done = False\n",
    "        ep = Episode()\n",
    "        while not done:\n",
    "            action = env_test.action_space.sample()\n",
    "            observation_prime, reward, terminated, truncated, _ = env_test.step(action)\n",
    "        \n",
    "            ep.add_step(\n",
    "                observation.squeeze(),\n",
    "                action, \n",
    "                reward, \n",
    "                observation_prime.squeeze(),\n",
    "            )\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            observation = observation_prime\n",
    "        \n",
    "        buffer.store(ep)\n",
    "            \n",
    "    return buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        env,\n",
    "        in_channels=1, \n",
    "        hidden_filters=list([32, 64, 64]),\n",
    "        hidden_size=512,\n",
    "        start_epsilon = 0.99,\n",
    "        max_decay = 0.1,\n",
    "        decay_steps = 1000,  \n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.epsilon = start_epsilon\n",
    "        self.max_decay = max_decay\n",
    "        self.decay_steps = decay_steps\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        self.layers_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_filters[0], kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_filters[0], hidden_filters[1], kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_filters[1], hidden_filters[2], kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_filters[-1] * 7 * 7, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, self.num_actions)\n",
    "        \n",
    "        self.apply(self._init)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x_view = x.view(-1, C, H, W)\n",
    "        x_view = self.layers_cnn(x_view / 255.0)\n",
    "        \n",
    "        x_view = x_view.reshape(B, T, -1)\n",
    "        hidden_states, _ = self.lstm(x_view)\n",
    "        output = self.linear(hidden_states)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "       \n",
    "    def _init(self, m):\n",
    "      if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "          nn.init.zeros_(m.bias)\n",
    "              \n",
    "    def epsilon_greedy(self, state, dim=-1):\n",
    "        rng = np.random.random()\n",
    "\n",
    "        if rng < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "            action = torch.tensor(action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self(state)\n",
    "            \n",
    "            action = torch.argmax(q_values.squeeze(), dim=dim)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self, step):\n",
    "        self.epsilon = self.max_decay + (self.start_epsilon - self.max_decay) * max(0, (self.decay_steps - step) / self.decay_steps)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 18])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DRQN(env_test, start_epsilon=0.0).to(device)\n",
    "rand = torch.randn((1, 1, 1, 84, 84)).to(device)\n",
    "net(rand).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricTrackerDRQN:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.rewards = deque(maxlen=window_size)\n",
    "        \n",
    "    def add_episode_reward(self, reward):\n",
    "        self.current_episode_reward += reward\n",
    "        \n",
    "    @property\n",
    "    def avg_reward(self):\n",
    "        return np.mean(self.rewards) if self.rewards else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env, \n",
    "    name, \n",
    "    q_network, \n",
    "    target_network, \n",
    "    timesteps, \n",
    "    replay, \n",
    "    train_freq, \n",
    "    batch_size, \n",
    "    gamma, \n",
    "    decay_start,\n",
    "    C,\n",
    "    save_step=850000,\n",
    "):\n",
    "    loss_func = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    episode_count = 0\n",
    "    best_avg_reward = -float('inf')\n",
    "    metrics = MetricTracker()\n",
    "    \n",
    "    optimizer = torch.optim.RMSprop(q_network.params(), lr=2.5e-4, momentum=0.95, alpha=0.95, eps=0.01)\n",
    "    \n",
    "    for step in range(1, timesteps+1):\n",
    "        \n",
    "        # sample full episode \n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode = Episode()\n",
    "        while not done:\n",
    "            batched_obs = obs.reshape(1, 1, 1, 84, 84)\n",
    "            action = q_network.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32, device=device)).cpu().item()\n",
    "            obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            episode.add_step(\n",
    "                obs, \n",
    "                action,\n",
    "                reward, \n",
    "                obs_prime,\n",
    "                terminated or truncated\n",
    "            )\n",
    "            \n",
    "            obs = obs_prime\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        replay.store(episode)\n",
    "        \n",
    "        if step % train_freq == 0:\n",
    "            pass # do training here\n",
    "        \n",
    "        if step > decay_start: \n",
    "            q_network.epsilon_decay(step)\n",
    "            target_network.epsilon_decay(step)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        steps_per_sec = step / elapsed_time\n",
    "        metrics.end_episode()\n",
    "        episode_count += 1\n",
    "        \n",
    "        if metrics.avg_reward > best_avg_reward and step > save_step:\n",
    "            best_avg_reward = metrics.avg_reward\n",
    "            torch.save({\n",
    "                'step': step,\n",
    "                'model_state_dict': q_network.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'avg_reward': metrics.avg_reward,\n",
    "            }, f\"models/{name}_dqn_best_{step}.pth\")\n",
    "            \n",
    "        print(f\"\\rStep: {step:,}/{timesteps:,} | \"\n",
    "                f\"Episodes: {episode_count} | \"\n",
    "                f\"Avg Reward: {metrics.avg_reward:.1f} | \"\n",
    "                f\"Epsilon: {q_network.epsilon:.3f} | \"\n",
    "                f\"Steps/sec: {steps_per_sec:.1f}\", end=\"\\r\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

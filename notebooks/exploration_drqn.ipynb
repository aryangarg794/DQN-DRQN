{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Recurrent Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- https://arxiv.org/pdf/1507.06527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import minatar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers.frame_stack import LazyFrames\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    AtariWrapper,\n",
    "    FireResetEnv,\n",
    ")\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "gym.register_envs(minatar)\n",
    "\n",
    "from utils.replay import LazyFramesToNumpyWrapper, MetricTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(frame, gray=False):\n",
    "    if gray:\n",
    "        plt.imshow(frame, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_multiple_frames(frames):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(frames[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickeringAtari(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def observation(self, observation, prob=0.3):\n",
    "        if np.random.rand() < prob:\n",
    "            return np.zeros_like(observation, dtype=np.uint8)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(game, render='rgb_array'):\n",
    "    env = gym.make(game, render_mode=render)\n",
    "    env = AtariWrapper(env, terminal_on_life_loss=False)\n",
    "    env = LazyFramesToNumpyWrapper(env)\n",
    "    env = FlickeringAtari(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = make_env('BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, _ = env_test.reset()\n",
    "print(observation.shape)\n",
    "display_frame(observation, gray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Episode:\n",
    "    \n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.obs_prime = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add_step(\n",
    "        self,\n",
    "        observation,\n",
    "        action, \n",
    "        reward, \n",
    "        observation_prime, \n",
    "        terminated\n",
    "    ):\n",
    "        self.obs.append(observation)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.obs_prime.append(observation_prime)\n",
    "        self.dones.append(terminated)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, slice):\n",
    "            ep = Episode()\n",
    "            ep.obs = self.obs[index]\n",
    "            ep.actions = self.actions[index]\n",
    "            ep.rewards = self.rewards[index]\n",
    "            ep.obs_prime = self.obs_prime[index]\n",
    "            ep.dones = self.dones[index]\n",
    "            return ep\n",
    "        elif isinstance(index, tuple):\n",
    "            arg = index[0]\n",
    "            pos = index[1]\n",
    "            if arg == 0: \n",
    "                return self.obs[pos]\n",
    "            elif arg == 1:\n",
    "                return self.actions[pos]\n",
    "            elif arg == 2:\n",
    "                return self.rewards[pos]\n",
    "            elif arg == 3:\n",
    "                return self.obs_prime[pos]\n",
    "            elif arg == 4:\n",
    "                return self.dones[pos]\n",
    "         \n",
    "        return (\n",
    "            self.obs[index],\n",
    "            self.actions[index],\n",
    "            self.rewards[index],\n",
    "            self.obs_prime[index],\n",
    "            self.dones[index]\n",
    "        )\n",
    "    \n",
    "    def pad(self, stop):\n",
    "        new_ep = Episode()\n",
    "        new_ep.obs = deepcopy(self.obs)\n",
    "        new_ep.actions = deepcopy(self.actions)\n",
    "        new_ep.rewards = deepcopy(self.rewards)\n",
    "        new_ep.obs_prime = deepcopy(self.obs_prime)\n",
    "        new_ep.dones = deepcopy(self.dones)\n",
    "        def pad_field(arr, stop, zeros, random_start=None):\n",
    "            copy_arr = deepcopy(arr)\n",
    "            if len(arr) > stop:\n",
    "                copy_arr = copy_arr[random_start:random_start+stop]\n",
    "            else:\n",
    "                for i in range(stop):\n",
    "                    if i >= len(copy_arr):\n",
    "                        copy_arr.append(zeros)\n",
    "            return copy_arr\n",
    "            \n",
    "        random_start = np.random.randint(low=0, high=(len(new_ep.obs)-stop-1))\n",
    "        new_ep.obs = pad_field(new_ep.obs, stop, zeros=np.zeros_like(self.obs[0]), random_start=random_start)\n",
    "        new_ep.actions = pad_field(new_ep.actions, stop, zeros=np.int64(0), random_start=random_start)\n",
    "        new_ep.rewards = pad_field(new_ep.rewards, stop, zeros=np.float32(0.0), random_start=random_start)\n",
    "        new_ep.obs_prime = pad_field(new_ep.obs_prime, stop, zeros=np.zeros_like(self.obs_prime[0]), random_start=random_start)\n",
    "        new_ep.dones = pad_field(new_ep.dones, stop, zeros=True, random_start=random_start)\n",
    "        \n",
    "        return new_ep\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.obs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Episode of Length {len(self)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Episode of Length 120"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, _ = env_test.reset()\n",
    "done = False\n",
    "ep = Episode()\n",
    "while not done:\n",
    "    action = env_test.action_space.sample()\n",
    "    observation_prime, reward, terminated, truncated, _ = env_test.step(action) \n",
    "    ep.add_step(\n",
    "        observation.squeeze(),\n",
    "        action, \n",
    "        reward, \n",
    "        observation_prime.squeeze(),\n",
    "        terminated or truncated\n",
    "    )\n",
    "    \n",
    "    done = terminated or truncated\n",
    "    observation = observation_prime\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Episode of Length 120, Episode of Length 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ep = ep.pad(100)\n",
    "ep, padded_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayBufferDRQN:\n",
    "    \n",
    "    def __init__(self, capacity, device='cuda'):\n",
    "        self._buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "    \n",
    "    def store(self, episode):\n",
    "        self._buffer.append(episode)\n",
    "    \n",
    "    def sample(self, batch_size, stop=None):\n",
    "        batch = random.choices(self._buffer, k=batch_size)\n",
    "        max_len = len(max(batch, key=len)) if stop is None else stop\n",
    "        padded_batch = [ep.pad(max_len) for ep in batch]\n",
    "        \n",
    "        return (\n",
    "            self.extract_item(padded_batch, 0, (batch_size, max_len, 1, 84, 84), dtype=torch.float32),\n",
    "            self.extract_item(padded_batch, 1, (batch_size, max_len, 1), dtype=torch.int64),\n",
    "            self.extract_item(padded_batch, 2, (batch_size, max_len, 1), dtype=torch.float32),\n",
    "            self.extract_item(padded_batch, 3, (batch_size, max_len, 1, 84, 84), dtype=torch.float32),\n",
    "            self.extract_item(padded_batch, 4, (batch_size, max_len, 1), dtype=torch.bool)\n",
    "        )\n",
    "\n",
    "    def extract_item(self, batch, arg, shape, dtype):\n",
    "        batched_values = np.array([val[arg, :] for val in batch]).reshape(shape)\n",
    "        batched_values = torch.as_tensor(batched_values, dtype=dtype, device=self.device)\n",
    "        return batched_values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._buffer[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBufferDRQN(10)\n",
    "\n",
    "observation, _ = env_test.reset()\n",
    "for _ in range(10):    \n",
    "    observation, _ = env_test.reset()\n",
    "    done = False\n",
    "    ep = Episode()\n",
    "    while not done:\n",
    "        action = env_test.action_space.sample()\n",
    "        observation_prime, reward, terminated, truncated, _ = env_test.step(action) \n",
    "        ep.add_step(\n",
    "            observation.squeeze(),\n",
    "            action, \n",
    "            reward, \n",
    "            observation_prime.squeeze(),\n",
    "            terminated or truncated\n",
    "        )\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        observation = observation_prime\n",
    "        \n",
    "    buffer.store(ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    test_obs, test_act, test_rew, test_prime, test_dones = buffer.sample(16, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 100, 1, 84, 84]),\n",
       " torch.Size([16, 100, 1]),\n",
       " torch.Size([16, 100, 1]),\n",
       " torch.Size([16, 100, 1, 84, 84]),\n",
       " torch.Size([16, 100, 1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_obs.shape, test_act.shape, test_rew.shape, test_prime.shape, test_dones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_buffer(preload, capacity, game):\n",
    "    env = make_env(game)\n",
    "    buffer = ReplayBufferDRQN(capacity)\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    for _ in tqdm(range(preload)):    \n",
    "        observation, _ = env.reset()\n",
    "        done = False\n",
    "        ep = Episode()\n",
    "        while not done:\n",
    "            action = env_test.action_space.sample()\n",
    "            observation_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            ep.add_step(\n",
    "                observation.squeeze(),\n",
    "                action, \n",
    "                reward, \n",
    "                observation_prime.squeeze(),\n",
    "                terminated or truncated\n",
    "            )\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            observation = observation_prime\n",
    "        \n",
    "        buffer.store(ep)\n",
    "            \n",
    "    return buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRQN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        env,\n",
    "        in_channels=1, \n",
    "        hidden_filters=list([32, 64, 64]),\n",
    "        hidden_size=512,\n",
    "        start_epsilon = 0.99,\n",
    "        max_decay = 0.1,\n",
    "        device='cuda',\n",
    "        decay_steps = 1000,  \n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.env = env\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.epsilon = start_epsilon\n",
    "        self.max_decay = max_decay\n",
    "        self.decay_steps = decay_steps\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        \n",
    "        \n",
    "        self.layers_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_filters[0], kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_filters[0], hidden_filters[1], kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_filters[1], hidden_filters[2], kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_filters[-1] * 7 * 7, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, self.num_actions)\n",
    "        \n",
    "        self.apply(self._init)\n",
    "    \n",
    "    def forward(self, x, h0, c0):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x_view = x.view(-1, C, H, W)\n",
    "        x_view = self.layers_cnn(x_view / 255.0)\n",
    "        \n",
    "        x_view = x_view.reshape(B, T, -1)\n",
    "        hidden_states, _ = self.lstm(x_view, (h0, c0))\n",
    "        output = self.linear(hidden_states)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "       \n",
    "    def _init(self, m):\n",
    "      if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "          nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def init_hidden_state(self, batch_size, training=True):\n",
    "        if training:\n",
    "            return torch.zeros((1, batch_size, self.hidden_size), device=self.device), torch.zeros((1, batch_size, self.hidden_size), device=self.device)\n",
    "        else:\n",
    "            return torch.zeros([1, 1, self.hidden_size], device=self.device), torch.zeros((1, 1, self.hidden_size), device=self.device)\n",
    "              \n",
    "    def epsilon_greedy(self, state, dim=-1):\n",
    "        rng = np.random.random()\n",
    "\n",
    "        if rng < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "            action = torch.tensor(action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                h_0, c_0 = self.init_hidden_state(1)\n",
    "                q_values = self(state, h_0, c_0)\n",
    "            \n",
    "            action = torch.argmax(q_values.squeeze(), dim=dim)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self, step):\n",
    "        self.epsilon = self.max_decay + (self.start_epsilon - self.max_decay) * max(0, (self.decay_steps - step) / self.decay_steps)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DRQN(env_test, start_epsilon=0.9, decay_steps=10).to(device)\n",
    "h0, c0 = net.init_hidden_state(16)\n",
    "out = net(test_prime, h0, c0)\n",
    "maxes = torch.amax(out, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricTrackerDRQN:\n",
    "    def __init__(self, window_size=15):\n",
    "        self.window_size = window_size\n",
    "        self.rewards = deque(maxlen=window_size)\n",
    "        self.current_episode_reward = 0\n",
    "        \n",
    "    def add_episode_reward(self, reward):\n",
    "        self.current_episode_reward += reward\n",
    "        \n",
    "    def end_episode(self):\n",
    "        self.rewards.append(self.current_episode_reward)   \n",
    "        self.current_episode_reward = 0\n",
    "         \n",
    "    @property\n",
    "    def avg_reward(self):\n",
    "        return np.mean(self.rewards) if self.rewards else 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env, \n",
    "    name, \n",
    "    q_network, \n",
    "    target_network, \n",
    "    timesteps, \n",
    "    replay, \n",
    "    train_freq, \n",
    "    batch_size, \n",
    "    lr,\n",
    "    gamma, \n",
    "    episode_len,\n",
    "    decay_start,\n",
    "    C,\n",
    "    save_step,\n",
    "):\n",
    "    loss_func = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    episode_count = 0\n",
    "    best_avg_reward = -float('inf')\n",
    "    metrics = MetricTrackerDRQN()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
    "    obs, _ = env.reset()\n",
    "    episode = Episode()\n",
    "    steps_per_sec = 0\n",
    "    \n",
    "    for step in range(1, timesteps+1):\n",
    "        \n",
    "        batched_obs = obs.reshape(1, 1, 1, 84, 84)\n",
    "        action = q_network.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32, device=device)).cpu().item()\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        episode.add_step(\n",
    "            obs.squeeze(), \n",
    "            action,\n",
    "            reward, \n",
    "            obs_prime.squeeze(),\n",
    "            terminated or truncated\n",
    "        )\n",
    "        \n",
    "        metrics.add_episode_reward(reward)\n",
    "        obs = obs_prime\n",
    "        \n",
    "        \n",
    "        if step % train_freq == 0:\n",
    "            observations, actions, rewards, observation_primes, dones = replay.sample(batch_size, episode_len)\n",
    "            with torch.no_grad():\n",
    "                h_target, c_target = target_network.init_hidden_state(batch_size)\n",
    "                q_values_minus = target_network(observation_primes, h_target, c_target)\n",
    "                boostrapped_values = torch.amax(q_values_minus, dim=-1, keepdim=True)\n",
    "                \n",
    "            y_trues = torch.where(dones, rewards, rewards + gamma * boostrapped_values)\n",
    "            h_0, c_0 = q_network.init_hidden_state(batch_size)\n",
    "            y_preds = q_network(observations, h_0, c_0)\n",
    "\n",
    "            loss = loss_func(y_preds.gather(-1, actions), y_trues)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(q_network.parameters(), 10)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if step > decay_start: \n",
    "            q_network.epsilon_decay(step)\n",
    "            target_network.epsilon_decay(step)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            steps_per_sec = step / elapsed_time\n",
    "            metrics.end_episode()\n",
    "            episode_count += 1\n",
    "            \n",
    "            replay.store(episode)\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "            episode = Episode()\n",
    "        \n",
    "            if metrics.avg_reward > best_avg_reward and step > save_step:\n",
    "                best_avg_reward = metrics.avg_reward\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'model_state_dict': q_network.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'avg_reward': metrics.avg_reward,\n",
    "                }, f\"models/{name}_drqn_best_{step}.pth\")\n",
    "                \n",
    "        print(f\"\\rStep: {step:,}/{timesteps:,} | \"\n",
    "                f\"Episodes: {episode_count} | \"\n",
    "                f\"Avg Reward: {metrics.avg_reward:.1f} | \"\n",
    "                f\"Epsilon: {q_network.epsilon:.3f} | \"\n",
    "                f\"Steps/sec: {steps_per_sec:.1f}\", end=\"\\r\")\n",
    "        \n",
    "        if step % C == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 250000\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "C = 500\n",
    "GAMMA = 0.99\n",
    "TRAIN_FREQ = 4\n",
    "DECAY_START = 50000 \n",
    "FINAL_ANNEAL = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:08<00:00,  7.32it/s]\n"
     ]
    }
   ],
   "source": [
    "buffer, env = load_buffer(500, 2000, game='BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 130,779/250,000 | Episodes: 724 | Avg Reward: 1.3 | Epsilon: 0.408 | Steps/sec: 16.5\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m target_network\u001b[38;5;241m.\u001b[39mload_state_dict(q_network\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m      5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m MetricTracker()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfrostbite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_network\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_FREQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGAMMA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecay_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDECAY_START\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150000\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m \n",
      "Cell \u001b[1;32mIn[54], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, name, q_network, target_network, timesteps, replay, train_freq, batch_size, lr, gamma, episode_len, decay_start, C, save_step)\u001b[0m\n\u001b[0;32m     43\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs_prime\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m train_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 47\u001b[0m     observations, actions, rewards, observation_primes, dones \u001b[38;5;241m=\u001b[39m \u001b[43mreplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     49\u001b[0m         h_target, c_target \u001b[38;5;241m=\u001b[39m target_network\u001b[38;5;241m.\u001b[39minit_hidden_state(batch_size)\n",
      "Cell \u001b[1;32mIn[40], line 15\u001b[0m, in \u001b[0;36mReplayBufferDRQN.sample\u001b[1;34m(self, batch_size, stop)\u001b[0m\n\u001b[0;32m     13\u001b[0m batch \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer, k\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     14\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mmax\u001b[39m(batch, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m stop\n\u001b[1;32m---> 15\u001b[0m padded_batch \u001b[38;5;241m=\u001b[39m [\u001b[43mep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_item(padded_batch, \u001b[38;5;241m0\u001b[39m, (batch_size, max_len, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_item(padded_batch, \u001b[38;5;241m1\u001b[39m, (batch_size, max_len, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_item(padded_batch, \u001b[38;5;241m4\u001b[39m, (batch_size, max_len, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m     23\u001b[0m )\n",
      "Cell \u001b[1;32mIn[37], line 80\u001b[0m, in \u001b[0;36mEpisode.pad\u001b[1;34m(self, stop)\u001b[0m\n\u001b[0;32m     78\u001b[0m new_ep\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m pad_field(new_ep\u001b[38;5;241m.\u001b[39mactions, stop, zeros\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64(\u001b[38;5;241m0\u001b[39m), random_start\u001b[38;5;241m=\u001b[39mrandom_start)\n\u001b[0;32m     79\u001b[0m new_ep\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m pad_field(new_ep\u001b[38;5;241m.\u001b[39mrewards, stop, zeros\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32(\u001b[38;5;241m0.0\u001b[39m), random_start\u001b[38;5;241m=\u001b[39mrandom_start)\n\u001b[1;32m---> 80\u001b[0m new_ep\u001b[38;5;241m.\u001b[39mobs_prime \u001b[38;5;241m=\u001b[39m \u001b[43mpad_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_ep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeros\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_prime\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m new_ep\u001b[38;5;241m.\u001b[39mdones \u001b[38;5;241m=\u001b[39m pad_field(new_ep\u001b[38;5;241m.\u001b[39mdones, stop, zeros\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_start\u001b[38;5;241m=\u001b[39mrandom_start)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_ep\n",
      "Cell \u001b[1;32mIn[37], line 67\u001b[0m, in \u001b[0;36mEpisode.pad.<locals>.pad_field\u001b[1;34m(arr, stop, zeros, random_start)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_field\u001b[39m(arr, stop, zeros, random_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 67\u001b[0m     copy_arr \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr) \u001b[38;5;241m>\u001b[39m stop:\n\u001b[0;32m     69\u001b[0m         copy_arr \u001b[38;5;241m=\u001b[39m copy_arr[random_start:random_start\u001b[38;5;241m+\u001b[39mstop]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.5-windows-x86_64-none\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.5-windows-x86_64-none\\Lib\\copy.py:196\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    194\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m--> 196\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\uv\\python\\cpython-3.12.5-windows-x86_64-none\\Lib\\copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_network = DRQN(env, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network = DRQN(env, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "metrics = MetricTracker()\n",
    "\n",
    "train(\n",
    "    env=env,\n",
    "    q_network=q_network, \n",
    "    name='frostbite',\n",
    "    target_network=target_network, \n",
    "    timesteps=TIMESTEPS, \n",
    "    replay=buffer, \n",
    "    train_freq=TRAIN_FREQ, \n",
    "    lr=LR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    gamma=GAMMA, \n",
    "    episode_len=30, \n",
    "    decay_start=DECAY_START,\n",
    "    C=C,\n",
    "    save_step=150000\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(game, model, num_eps=2):\n",
    "    env_test = make_env(game, render='human')\n",
    "\n",
    "    q_network_trained = DRQN(env_test)\n",
    "    q_network_trained.load_state_dict(torch.load(model, weights_only=False)['model_state_dict'])\n",
    "    q_network_trained.eval()\n",
    "    q_network_trained.epsilon = 0.05\n",
    "    \n",
    "    \n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        print(f'Episode {episode}', end='\\r', flush=True)\n",
    "        obs, _ = env_test.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            batched_obs = np.expand_dims(obs.squeeze(), axis=0)\n",
    "            action = q_network_trained.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32)).cpu().item()\n",
    "                \n",
    "            next_observation, reward, terminated, truncated, _ = env_test.step(action)\n",
    "            total_reward += reward\n",
    "            obs = next_observation\n",
    "\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        rewards_list.append(total_reward)\n",
    "\n",
    "    env_test.close()\n",
    "    print(f'Average episode reward achieved: {np.mean(rewards_list)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNDonla6UtLL"
   },
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "- https://arxiv.org/pdf/1312.5602\n",
    "- https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:39:27.913282Z",
     "iopub.status.busy": "2024-11-11T23:39:27.912965Z",
     "iopub.status.idle": "2024-11-11T23:39:33.597815Z",
     "shell.execute_reply": "2024-11-11T23:39:33.597222Z",
     "shell.execute_reply.started": "2024-11-11T23:39:27.913260Z"
    },
    "id": "8p6bFDwGUtLQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers.frame_stack import LazyFrames\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    AtariWrapper,\n",
    "    FireResetEnv,\n",
    ")\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVsLT3IJUtLR"
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:39:33.599404Z",
     "iopub.status.busy": "2024-11-11T23:39:33.598856Z",
     "iopub.status.idle": "2024-11-11T23:39:33.603184Z",
     "shell.execute_reply": "2024-11-11T23:39:33.602543Z",
     "shell.execute_reply.started": "2024-11-11T23:39:33.599370Z"
    },
    "id": "SN1JaFMNUtLR"
   },
   "outputs": [],
   "source": [
    "def display_frame(frame, gray=False):\n",
    "    if gray:\n",
    "        plt.imshow(frame, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:39:34.923366Z",
     "iopub.status.busy": "2024-11-11T23:39:34.923022Z",
     "iopub.status.idle": "2024-11-11T23:39:34.927294Z",
     "shell.execute_reply": "2024-11-11T23:39:34.926641Z",
     "shell.execute_reply.started": "2024-11-11T23:39:34.923344Z"
    },
    "id": "kwBF80aEUtLS"
   },
   "outputs": [],
   "source": [
    "def display_multiple_frames(frames):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(frames[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make('PongNoFrameskip-v4', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, _ = env_test.reset()\n",
    "for _ in range(100):    \n",
    "    action = env_test.action_space.sample()\n",
    "    observation_prime, reward, terminated, truncated, _ = env_test.step(action)\n",
    "display_frame(observation_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFramesToNumpyWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if isinstance(observation, LazyFrames):\n",
    "            return np.array(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:51:03.780304Z",
     "iopub.status.busy": "2024-11-11T23:51:03.779976Z",
     "iopub.status.idle": "2024-11-11T23:51:03.784161Z",
     "shell.execute_reply": "2024-11-11T23:51:03.783388Z",
     "shell.execute_reply.started": "2024-11-11T23:51:03.780284Z"
    },
    "id": "t2iioEZWUtLS"
   },
   "outputs": [],
   "source": [
    "def make_env(game, render='rgb_array'):\n",
    "    env = gym.make(game, render_mode=render)\n",
    "    env = AtariWrapper(env, terminal_on_life_loss=False, frame_skip=4)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    env = LazyFramesToNumpyWrapper(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWyj-fAFUtLT"
   },
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:51:11.931867Z",
     "iopub.status.busy": "2024-11-11T23:51:11.931525Z",
     "iopub.status.idle": "2024-11-11T23:51:11.940279Z",
     "shell.execute_reply": "2024-11-11T23:51:11.939504Z",
     "shell.execute_reply.started": "2024-11-11T23:51:11.931845Z"
    },
    "id": "ti6IvoK8UtLT"
   },
   "outputs": [],
   "source": [
    "# some aspects of the replay buffer were inspired by https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained\n",
    "# since it was quite slow at the beginning\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.capacity = capacity\n",
    "        self._buffer =  np.zeros((capacity,), dtype=object)\n",
    "        self._position = 0\n",
    "        self._size = 0\n",
    "\n",
    "    def store(self, experience: tuple) -> None:\n",
    "        idx = self._position % self.capacity\n",
    "        self._buffer[idx] = experience\n",
    "        self._position += 1\n",
    "        self._size = min(self._size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, device='cuda'):\n",
    "        buffer = self._buffer[0:min(self._position-1, self.capacity-1)]\n",
    "        batch = np.random.choice(buffer, size=[batch_size], replace=True)\n",
    "        return (\n",
    "            self.transform(batch, 0, shape=(batch_size, 4, 84, 84), dtype=torch.float32, device=device),\n",
    "            self.transform(batch, 1, shape=(batch_size, 1), dtype=torch.int64, device=device),\n",
    "            self.transform(batch, 2, shape=(batch_size, 1), dtype=torch.float32, device=device),\n",
    "            self.transform(batch, 3, shape=(batch_size, 4, 84, 84), dtype=torch.float32, device=device),\n",
    "            self.transform(batch, 4, shape=(batch_size, 1), dtype=torch.bool, device=device)\n",
    "        )\n",
    "        \n",
    "    def transform(self, batch, index, shape, dtype, device):\n",
    "        batched_values = np.array([val[index] for val in batch]).reshape(shape)\n",
    "        batched_values = torch.as_tensor(batched_values, dtype=dtype, device=device)\n",
    "        return batched_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._buffer[index]\n",
    "\n",
    "    def __setitem__(self, index, value: tuple):\n",
    "        self._buffer[index] = value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T23:51:13.724500Z",
     "iopub.status.busy": "2024-11-11T23:51:13.724178Z",
     "iopub.status.idle": "2024-11-11T23:55:00.276479Z",
     "shell.execute_reply": "2024-11-11T23:55:00.275794Z",
     "shell.execute_reply.started": "2024-11-11T23:51:13.724481Z"
    },
    "id": "t1xMD1agUtLT",
    "outputId": "030b8261-979b-49ce-855e-a3161edc8925"
   },
   "outputs": [],
   "source": [
    "def load_buffer(preload, capacity, game):\n",
    "    env = make_env(game)\n",
    "    buffer = ReplayBuffer(capacity)\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    for _ in range(preload):    \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        observation_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        buffer.store((\n",
    "            observation.squeeze(), \n",
    "            action, \n",
    "            reward, \n",
    "            observation_prime.squeeze(), \n",
    "            terminated or truncated))\n",
    "        observation = observation_prime\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            observation, _ = env.reset()\n",
    "    \n",
    "    print(len(buffer))        \n",
    "    return buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garga\\AppData\\Local\\Temp\\ipykernel_3188\\2204954719.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  return np.array(observation)\n"
     ]
    }
   ],
   "source": [
    "test_buffer, env_test = load_buffer(100, 100, 'PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_multiple_frames(test_buffer[10][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, _ = env_test.reset()\n",
    "for _ in range(100):    \n",
    "    action = env_test.action_space.sample()\n",
    "    observation_prime, reward, terminated, truncated, _ = env_test.step(action)\n",
    "    test_buffer.store((observation.squeeze(), action, reward, observation_prime.squeeze(), terminated or truncated))\n",
    "    observation = observation_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T23:55:00.277795Z",
     "iopub.status.busy": "2024-11-11T23:55:00.277460Z",
     "iopub.status.idle": "2024-11-11T23:55:00.370240Z",
     "shell.execute_reply": "2024-11-11T23:55:00.369659Z",
     "shell.execute_reply.started": "2024-11-11T23:55:00.277776Z"
    },
    "id": "4naELqhxUtLT",
    "outputId": "8ae8045c-0fd7-4da5-8ec8-393fd79866bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = test_buffer[10][3]\n",
    "print(frame.shape)\n",
    "display_multiple_frames(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T00:28:15.751279Z",
     "iopub.status.busy": "2024-11-12T00:28:15.750977Z",
     "iopub.status.idle": "2024-11-12T00:28:15.759495Z",
     "shell.execute_reply": "2024-11-12T00:28:15.758873Z",
     "shell.execute_reply.started": "2024-11-12T00:28:15.751258Z"
    },
    "id": "1BarzFKfUtLU"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        in_channels = 4,\n",
    "        hidden_filters = [16, 32],\n",
    "        start_epsilon = 0.99,\n",
    "        max_decay = 0.1,\n",
    "        decay_steps = 1000,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.epsilon = start_epsilon\n",
    "        self.max_decay = max_decay\n",
    "        self.decay_steps = decay_steps\n",
    "        self.env = env\n",
    "        self.num_actions = env.action_space.n \n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_filters[0], kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_filters[0], hidden_filters[1], kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(hidden_filters[1] * 9 * 9, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init)\n",
    "\n",
    "    def _init(self, m):\n",
    "      if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "          nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x / 255.0)\n",
    "\n",
    "    def epsilon_greedy(self, state, dim=1):\n",
    "        rng = np.random.random()\n",
    "\n",
    "        if rng < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "            action = torch.tensor(action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self(state)\n",
    "            \n",
    "            action = torch.argmax(q_values, dim=dim)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self, step):\n",
    "        self.epsilon = self.max_decay + (self.start_epsilon - self.max_decay) * max(0, (self.decay_steps - step) / self.decay_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricTracker:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.rewards = deque(maxlen=window_size)\n",
    "        self.episode_lengths = deque(maxlen=window_size)\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    def add_step_reward(self, reward):\n",
    "        self.current_episode_reward += reward\n",
    "        self.current_episode_length += 1\n",
    "        \n",
    "    def end_episode(self):\n",
    "        self.rewards.append(self.current_episode_reward)\n",
    "        self.episode_lengths.append(self.current_episode_length)\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    @property\n",
    "    def avg_reward(self):\n",
    "        return np.mean(self.rewards) if self.rewards else 0\n",
    "        \n",
    "    @property\n",
    "    def avg_episode_length(self):\n",
    "        return np.mean(self.episode_lengths) if self.episode_lengths else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PhWZY8OUtLU"
   },
   "source": [
    "Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env, \n",
    "    name, \n",
    "    q_network, \n",
    "    target_network, \n",
    "    optimizer, \n",
    "    timesteps, \n",
    "    replay, \n",
    "    metrics, \n",
    "    train_freq, \n",
    "    batch_size, \n",
    "    gamma, \n",
    "    decay_start,\n",
    "    C,\n",
    "    best_avg_reward,\n",
    "    save_step=850000,\n",
    "):\n",
    "    loss_func = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    episode_count = 0\n",
    "    best_avg_reward = -float('inf')\n",
    "        \n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    for step in range(1, timesteps+1):\n",
    "        batched_obs = np.expand_dims(obs.squeeze(), axis=0)\n",
    "        action = q_network.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32, device=device)).cpu().item()\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        \n",
    "        replay.store((obs.squeeze(), action, reward, obs_prime.squeeze(), terminated or truncated))\n",
    "        metrics.add_step_reward(reward)\n",
    "        obs = obs_prime\n",
    "\n",
    "        if step % train_freq == 0:\n",
    "            observations, actions, rewards, observation_primes, dones = replay.sample(batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values_minus = target_network(observation_primes)\n",
    "                boostrapped_values = torch.amax(q_values_minus, dim=1, keepdim=True)\n",
    "\n",
    "            y_trues = torch.where(dones, rewards, rewards + gamma * boostrapped_values)\n",
    "            y_preds = q_network(observations)\n",
    "\n",
    "            loss = loss_func(y_preds.gather(1, actions), y_trues)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "        if step > decay_start: \n",
    "            q_network.epsilon_decay(step)\n",
    "            target_network.epsilon_decay(step)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            steps_per_sec = step / elapsed_time\n",
    "            metrics.end_episode()\n",
    "            episode_count += 1\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "            \n",
    "            if metrics.avg_reward > best_avg_reward and step > save_step:\n",
    "                best_avg_reward = metrics.avg_reward\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'model_state_dict': q_network.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'avg_reward': metrics.avg_reward,\n",
    "                }, f\"models/{name}_dqn_best_{step}.pth\")\n",
    "                \n",
    "            print(f\"\\rStep: {step:,}/{timesteps:,} | \"\n",
    "                    f\"Episodes: {episode_count} | \"\n",
    "                    f\"Avg Reward: {metrics.avg_reward:.1f} | \"\n",
    "                    f\"Epsilon: {q_network.epsilon:.3f} | \"\n",
    "                    f\"Steps/sec: {steps_per_sec:.1f}\", end=\"\\r\")\n",
    "            \n",
    "        if step % C == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 6000000\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 64\n",
    "C = 10000\n",
    "GAMMA = 0.99\n",
    "TRAIN_FREQ = 4\n",
    "DECAY_START = 0 \n",
    "FINAL_ANNEAL = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "buffer_pong, env_pong = load_buffer(50000, 150000, game='PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5,999,953/6,000,000 | Episodes: 3033 | Avg Reward: 16.4 | Epsilon: 0.100 | Steps/sec: 326.8\r"
     ]
    }
   ],
   "source": [
    "q_network_pong = DQN(env_pong, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_pong = DQN(env_pong, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_pong.load_state_dict(q_network_pong.state_dict())\n",
    "optimizer_pong = torch.optim.Adam(q_network_pong.parameters(), lr=LR)\n",
    "\n",
    "metrics = MetricTracker()\n",
    "\n",
    "train(\n",
    "    env=env_pong,\n",
    "    q_network=q_network_pong, \n",
    "    name='pong',\n",
    "    target_network=target_network_pong, \n",
    "    optimizer=optimizer_pong, \n",
    "    timesteps=TIMESTEPS, \n",
    "    replay=buffer_pong, \n",
    "    metrics=metrics, \n",
    "    train_freq=TRAIN_FREQ, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    gamma=GAMMA, \n",
    "    decay_start=DECAY_START,\n",
    "    C=C,\n",
    "    best_avg_reward=-float('inf'),\n",
    "    save_step=5700000 # dont want to save too many models :P\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(game, model, num_eps=2):\n",
    "    env_test = make_env(game, render='human')\n",
    "\n",
    "    q_network_trained = DQN(env_test)\n",
    "    q_network_trained.load_state_dict(torch.load(model, weights_only=False)['model_state_dict'])\n",
    "    q_network_trained.eval()\n",
    "    q_network_trained.epsilon = 0.05\n",
    "    \n",
    "    \n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        print(f'Episode {episode}', end='\\r', flush=True)\n",
    "        obs, _ = env_test.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            batched_obs = np.expand_dims(obs.squeeze(), axis=0)\n",
    "            action = q_network_trained.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32)).cpu().item()\n",
    "                \n",
    "            next_observation, reward, terminated, truncated, _ = env_test.step(action)\n",
    "            total_reward += reward\n",
    "            obs = next_observation\n",
    "\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        rewards_list.append(total_reward)\n",
    "\n",
    "    env_test.close()\n",
    "    print(f'Average episode reward achieved: {np.mean(rewards_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garga\\AppData\\Local\\Temp\\ipykernel_36024\\2204954719.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  return np.array(observation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward achieved: 18.5\n"
     ]
    }
   ],
   "source": [
    "test('PongNoFrameskip-v4', 'models/pong_dqn_best_6M.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets train Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "buffer_breakout, env_breakout = load_buffer(50000, 150000, 'BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 4000000\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 64\n",
    "C = 6000\n",
    "GAMMA = 0.99\n",
    "TRAIN_FREQ = 4\n",
    "DECAY_START = 200000\n",
    "FINAL_ANNEAL = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3,999,245/4,000,000 | Episodes: 8198 | Avg Reward: 24.3 | Epsilon: 0.100 | Steps/sec: 316.1\r"
     ]
    }
   ],
   "source": [
    "q_network_breakout = DQN(env_breakout, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_breakout = DQN(env_breakout, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_breakout.load_state_dict(q_network_breakout.state_dict())\n",
    "optimizer_breakout = torch.optim.Adam(q_network_breakout.parameters(), lr=LR)\n",
    "\n",
    "metrics_breakout = MetricTracker() \n",
    "\n",
    "train(\n",
    "    env=env_breakout,\n",
    "    name='breakout',\n",
    "    q_network=q_network_breakout, \n",
    "    target_network=target_network_breakout, \n",
    "    optimizer=optimizer_breakout, \n",
    "    timesteps=TIMESTEPS, \n",
    "    replay=buffer_breakout, \n",
    "    metrics=metrics_breakout, \n",
    "    train_freq=TRAIN_FREQ, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    gamma=GAMMA, \n",
    "    decay_start=DECAY_START,\n",
    "    C=C,\n",
    "    best_avg_reward=-float('inf'),\n",
    "    save_step=4700000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward achieved: 28.0\n"
     ]
    }
   ],
   "source": [
    "test('BreakoutNoFrameskip-v4', 'models/breakout_dqn_best_4M.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

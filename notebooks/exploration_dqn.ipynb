{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNDonla6UtLL"
   },
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "- https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
    "- https://arxiv.org/pdf/1312.5602\n",
    "- https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:39:27.913282Z",
     "iopub.status.busy": "2024-11-11T23:39:27.912965Z",
     "iopub.status.idle": "2024-11-11T23:39:33.597815Z",
     "shell.execute_reply": "2024-11-11T23:39:33.597222Z",
     "shell.execute_reply.started": "2024-11-11T23:39:27.913260Z"
    },
    "id": "8p6bFDwGUtLQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers.frame_stack import LazyFrames\n",
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    AtariWrapper,\n",
    "    FireResetEnv,\n",
    ")\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVsLT3IJUtLR"
   },
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:39:33.599404Z",
     "iopub.status.busy": "2024-11-11T23:39:33.598856Z",
     "iopub.status.idle": "2024-11-11T23:39:33.603184Z",
     "shell.execute_reply": "2024-11-11T23:39:33.602543Z",
     "shell.execute_reply.started": "2024-11-11T23:39:33.599370Z"
    },
    "id": "SN1JaFMNUtLR"
   },
   "outputs": [],
   "source": [
    "def display_frame(frame, gray=False):\n",
    "    if gray:\n",
    "        plt.imshow(frame, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:39:34.923366Z",
     "iopub.status.busy": "2024-11-11T23:39:34.923022Z",
     "iopub.status.idle": "2024-11-11T23:39:34.927294Z",
     "shell.execute_reply": "2024-11-11T23:39:34.926641Z",
     "shell.execute_reply.started": "2024-11-11T23:39:34.923344Z"
    },
    "id": "kwBF80aEUtLS"
   },
   "outputs": [],
   "source": [
    "def display_multiple_frames(frames):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(frames[i], cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make('PongNoFrameskip-v4', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation, _ = env_test.reset()\n",
    "for _ in range(100):    \n",
    "    action = env_test.action_space.sample()\n",
    "    observation_prime, reward, terminated, truncated, _ = env_test.step(action)\n",
    "display_frame(observation_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyFramesToNumpyWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if isinstance(observation, LazyFrames):\n",
    "            return np.array(observation)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:51:03.780304Z",
     "iopub.status.busy": "2024-11-11T23:51:03.779976Z",
     "iopub.status.idle": "2024-11-11T23:51:03.784161Z",
     "shell.execute_reply": "2024-11-11T23:51:03.783388Z",
     "shell.execute_reply.started": "2024-11-11T23:51:03.780284Z"
    },
    "id": "t2iioEZWUtLS"
   },
   "outputs": [],
   "source": [
    "def make_env(game, render='rgb_array'):\n",
    "    env = gym.make(game, render_mode=render)\n",
    "    env = AtariWrapper(env, terminal_on_life_loss=False, frame_skip=4)\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    env = LazyFramesToNumpyWrapper(env)\n",
    "    if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWyj-fAFUtLT"
   },
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T23:51:11.931867Z",
     "iopub.status.busy": "2024-11-11T23:51:11.931525Z",
     "iopub.status.idle": "2024-11-11T23:51:11.940279Z",
     "shell.execute_reply": "2024-11-11T23:51:11.939504Z",
     "shell.execute_reply.started": "2024-11-11T23:51:11.931845Z"
    },
    "id": "ti6IvoK8UtLT"
   },
   "outputs": [],
   "source": [
    "# some aspects of the replay buffer were inspired by https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained\n",
    "# since it was quite slow at the beginning\n",
    "\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity) -> None:\n",
    "        self.capacity = capacity\n",
    "        self._buffer =  np.zeros((capacity,), dtype=object)\n",
    "        self._position = 0\n",
    "        self._size = 0\n",
    "\n",
    "    def store(self, experience: tuple) -> None:\n",
    "        idx = self._position % self.capacity\n",
    "        self._buffer[idx] = experience\n",
    "        self._position += 1\n",
    "        self._size = min(self._size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, device='cuda'):\n",
    "        buffer = self._buffer[0:min(self._position-1, self.capacity-1)]\n",
    "        batch = np.random.choice(buffer, size=[batch_size], replace=True)\n",
    "        return (\n",
    "            self.transform(batch, 0, shape=(batch_size, 4, 84, 84), dtype=torch.float32, device=device),\n",
    "            self.transform(batch, 1, shape=(batch_size, 1), dtype=torch.int64, device=device),\n",
    "            self.transform(batch, 2, shape=(batch_size, 1), dtype=torch.float32, device=device),\n",
    "            self.transform(batch, 3, shape=(batch_size, 4, 84, 84), dtype=torch.float32, device=device),\n",
    "            self.transform(batch, 4, shape=(batch_size, 1), dtype=torch.bool, device=device)\n",
    "        )\n",
    "        \n",
    "    def transform(self, batch, index, shape, dtype, device):\n",
    "        batched_values = np.array([val[index] for val in batch]).reshape(shape)\n",
    "        batched_values = torch.as_tensor(batched_values, dtype=dtype, device=device)\n",
    "        return batched_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._buffer[index]\n",
    "\n",
    "    def __setitem__(self, index, value: tuple):\n",
    "        self._buffer[index] = value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T23:51:13.724500Z",
     "iopub.status.busy": "2024-11-11T23:51:13.724178Z",
     "iopub.status.idle": "2024-11-11T23:55:00.276479Z",
     "shell.execute_reply": "2024-11-11T23:55:00.275794Z",
     "shell.execute_reply.started": "2024-11-11T23:51:13.724481Z"
    },
    "id": "t1xMD1agUtLT",
    "outputId": "030b8261-979b-49ce-855e-a3161edc8925"
   },
   "outputs": [],
   "source": [
    "def load_buffer(preload, capacity, game):\n",
    "    env = make_env(game)\n",
    "    buffer = ReplayBuffer(capacity)\n",
    "\n",
    "    observation, _ = env.reset()\n",
    "    for _ in range(preload):    \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        observation_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "        buffer.store((\n",
    "            observation.squeeze(), \n",
    "            action, \n",
    "            reward, \n",
    "            observation_prime.squeeze(), \n",
    "            terminated or truncated))\n",
    "        observation = observation_prime\n",
    "\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            observation, _ = env.reset()\n",
    "    \n",
    "    print(len(buffer))        \n",
    "    return buffer, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "test_buffer, _ = load_buffer(100, 1000, 'PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "execution": {
     "iopub.execute_input": "2024-11-11T23:55:00.277795Z",
     "iopub.status.busy": "2024-11-11T23:55:00.277460Z",
     "iopub.status.idle": "2024-11-11T23:55:00.370240Z",
     "shell.execute_reply": "2024-11-11T23:55:00.369659Z",
     "shell.execute_reply.started": "2024-11-11T23:55:00.277776Z"
    },
    "id": "4naELqhxUtLT",
    "outputId": "8ae8045c-0fd7-4da5-8ec8-393fd79866bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 84, 84)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frame = test_buffer[10][3]\n",
    "print(frame.shape)\n",
    "display_multiple_frames(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T00:28:15.751279Z",
     "iopub.status.busy": "2024-11-12T00:28:15.750977Z",
     "iopub.status.idle": "2024-11-12T00:28:15.759495Z",
     "shell.execute_reply": "2024-11-12T00:28:15.758873Z",
     "shell.execute_reply.started": "2024-11-12T00:28:15.751258Z"
    },
    "id": "1BarzFKfUtLU"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        in_channels = 4,\n",
    "        hidden_filters = [16, 32],\n",
    "        start_epsilon = 0.99,\n",
    "        max_decay = 0.1,\n",
    "        decay_steps = 1000,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.epsilon = start_epsilon\n",
    "        self.max_decay = max_decay\n",
    "        self.decay_steps = decay_steps\n",
    "        self.env = env\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_filters[0], kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_filters[0], hidden_filters[1], kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(hidden_filters[1] * 9 * 9, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "        self.apply(self._init)\n",
    "\n",
    "    def _init(self, m):\n",
    "      if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "          nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x / 255.0)\n",
    "\n",
    "    def epsilon_greedy(self, state, dim=1):\n",
    "        rng = np.random.random()\n",
    "\n",
    "        if rng < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "            action = torch.tensor(action)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self(state)\n",
    "            \n",
    "            action = torch.argmax(q_values, dim=dim)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def epsilon_decay(self, step):\n",
    "        self.epsilon = self.max_decay + (self.start_epsilon - self.max_decay) * max(0, (self.decay_steps - step) / self.decay_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricTracker:\n",
    "    def __init__(self, window_size=100):\n",
    "        self.window_size = window_size\n",
    "        self.rewards = deque(maxlen=window_size)\n",
    "        self.episode_lengths = deque(maxlen=window_size)\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    def add_step_reward(self, reward):\n",
    "        self.current_episode_reward += reward\n",
    "        self.current_episode_length += 1\n",
    "        \n",
    "    def end_episode(self):\n",
    "        self.rewards.append(self.current_episode_reward)\n",
    "        self.episode_lengths.append(self.current_episode_length)\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "        \n",
    "    @property\n",
    "    def avg_reward(self):\n",
    "        return np.mean(self.rewards) if self.rewards else 0\n",
    "        \n",
    "    @property\n",
    "    def avg_episode_length(self):\n",
    "        return np.mean(self.episode_lengths) if self.episode_lengths else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PhWZY8OUtLU"
   },
   "source": [
    "Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env, \n",
    "    name, \n",
    "    q_network, \n",
    "    target_network, \n",
    "    optimizer, \n",
    "    timesteps, \n",
    "    replay, \n",
    "    metrics, \n",
    "    train_freq, \n",
    "    batch_size, \n",
    "    gamma, \n",
    "    decay_start,\n",
    "    C,\n",
    "    best_avg_reward,\n",
    "    scheduler_step,\n",
    "    save_step=850000,\n",
    "):\n",
    "    loss_func = nn.MSELoss()\n",
    "    start_time = time.time()\n",
    "    episode_count = 0\n",
    "    best_avg_reward = -float('inf')\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=0.5)\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    for step in range(1, timesteps+1):\n",
    "        batched_obs = np.expand_dims(obs.squeeze(), axis=0)\n",
    "        action = q_network.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32, device=device)).cpu().item()\n",
    "        obs_prime, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        \n",
    "        replay.store((obs.squeeze(), action, reward, obs_prime.squeeze(), terminated or truncated))\n",
    "        metrics.add_step_reward(reward)\n",
    "        obs = obs_prime\n",
    "\n",
    "        if step % train_freq == 0:\n",
    "            observations, actions, rewards, observation_primes, dones = replay.sample(batch_size)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values_minus = target_network(observation_primes)\n",
    "                boostrapped_values = torch.amax(q_values_minus, dim=1, keepdim=True)\n",
    "\n",
    "            y_trues = torch.where(dones, rewards, rewards + gamma * boostrapped_values)\n",
    "            y_preds = q_network(observations)\n",
    "\n",
    "            loss = loss_func(y_preds.gather(1, actions), y_trues)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "            \n",
    "        if step > decay_start: \n",
    "            q_network.epsilon_decay(step)\n",
    "            target_network.epsilon_decay(step)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            steps_per_sec = step / elapsed_time\n",
    "            metrics.end_episode()\n",
    "            episode_count += 1\n",
    "            \n",
    "            obs, _ = env.reset()\n",
    "            \n",
    "            if metrics.avg_reward > best_avg_reward and step > save_step:\n",
    "                best_avg_reward = metrics.avg_reward\n",
    "                torch.save({\n",
    "                    'step': step,\n",
    "                    'model_state_dict': q_network.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'avg_reward': metrics.avg_reward,\n",
    "                }, f\"models/{name}_dqn_best_{step}.pth\")\n",
    "                \n",
    "            print(f\"\\rStep: {step:,}/{timesteps:,} | \"\n",
    "                    f\"Episodes: {episode_count} | \"\n",
    "                    f\"Avg Reward: {metrics.avg_reward:.1f} | \"\n",
    "                    f\"Epsilon: {q_network.epsilon:.3f} | \"\n",
    "                    f\"Steps/sec: {steps_per_sec:.1f}\", end=\"\\r\")\n",
    "            \n",
    "        if step % C == 0:\n",
    "            target_network.load_state_dict(q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 6000000\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 64\n",
    "C = 10000\n",
    "GAMMA = 0.99\n",
    "TRAIN_FREQ = 4\n",
    "DECAY_START = 0 \n",
    "FINAL_ANNEAL = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "buffer_pong, env_pong = load_buffer(50000, 150000, game='PongNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5,999,953/6,000,000 | Episodes: 3033 | Avg Reward: 16.4 | Epsilon: 0.100 | Steps/sec: 326.8\r"
     ]
    }
   ],
   "source": [
    "q_network_pong = DQN(env_pong, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_pong = DQN(env_pong, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_pong.load_state_dict(q_network_pong.state_dict())\n",
    "optimizer_pong = torch.optim.Adam(q_network_pong.parameters(), lr=LR)\n",
    "\n",
    "metrics = MetricTracker()\n",
    "\n",
    "train(\n",
    "    env=env_pong,\n",
    "    q_network=q_network_pong, \n",
    "    name='pong',\n",
    "    target_network=target_network_pong, \n",
    "    optimizer=optimizer_pong, \n",
    "    timesteps=TIMESTEPS, \n",
    "    replay=buffer_pong, \n",
    "    metrics=metrics, \n",
    "    train_freq=TRAIN_FREQ, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    gamma=GAMMA, \n",
    "    decay_start=DECAY_START,\n",
    "    C=C,\n",
    "    best_avg_reward=-float('inf'),\n",
    "    scheduler_step=4000000,\n",
    "    save_step=5700000 # dont want to save too many models :P\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(game, model, num_eps=2):\n",
    "    env_test = make_env(game, render='human')\n",
    "\n",
    "    q_network_trained = DQN(env_test)\n",
    "    q_network_trained.load_state_dict(torch.load(model, weights_only=False)['model_state_dict'])\n",
    "    q_network_trained.eval()\n",
    "    q_network_trained.epsilon = 0.05\n",
    "    \n",
    "    \n",
    "    rewards_list = []\n",
    "\n",
    "    for episode in range(num_eps):\n",
    "        print(f'Episode {episode}', end='\\r', flush=True)\n",
    "        obs, _ = env_test.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            batched_obs = np.expand_dims(obs.squeeze(), axis=0)\n",
    "            action = q_network_trained.epsilon_greedy(torch.as_tensor(batched_obs, dtype=torch.float32)).cpu().item()\n",
    "                \n",
    "            next_observation, reward, terminated, truncated, _ = env_test.step(action)\n",
    "            total_reward += reward\n",
    "            obs = next_observation\n",
    "\n",
    "            done = terminated or truncated\n",
    "            \n",
    "        rewards_list.append(total_reward)\n",
    "\n",
    "    env_test.close()\n",
    "    print(f'Average episode reward achieved: {np.mean(rewards_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garga\\AppData\\Local\\Temp\\ipykernel_36024\\2204954719.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.\n",
      "  return np.array(observation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward achieved: 18.5\n"
     ]
    }
   ],
   "source": [
    "test('PongNoFrameskip-v4', 'models/pong_dqn_best_6M.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets train Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "buffer_breakout, env_breakout = load_buffer(50000, 150000, 'BreakoutNoFrameskip-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 4000000\n",
    "LR = 2.5e-4\n",
    "BATCH_SIZE = 64\n",
    "C = 6000\n",
    "GAMMA = 0.99\n",
    "TRAIN_FREQ = 4\n",
    "DECAY_START = 200000\n",
    "FINAL_ANNEAL = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3,999,245/4,000,000 | Episodes: 8198 | Avg Reward: 24.3 | Epsilon: 0.100 | Steps/sec: 316.1\r"
     ]
    }
   ],
   "source": [
    "q_network_breakout = DQN(env_breakout, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_breakout = DQN(env_breakout, decay_steps=FINAL_ANNEAL).to(device)\n",
    "target_network_breakout.load_state_dict(q_network_breakout.state_dict())\n",
    "optimizer_breakout = torch.optim.Adam(q_network_breakout.parameters(), lr=LR)\n",
    "\n",
    "metrics_breakout = MetricTracker() \n",
    "\n",
    "train(\n",
    "    env=env_breakout,\n",
    "    name='breakout',\n",
    "    q_network=q_network_breakout, \n",
    "    target_network=target_network_breakout, \n",
    "    optimizer=optimizer_breakout, \n",
    "    timesteps=TIMESTEPS, \n",
    "    replay=buffer_breakout, \n",
    "    metrics=metrics_breakout, \n",
    "    train_freq=TRAIN_FREQ, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    gamma=GAMMA, \n",
    "    decay_start=DECAY_START,\n",
    "    C=C,\n",
    "    scheduler_step=3000000,\n",
    "    best_avg_reward=-float('inf'),\n",
    "    save_step=4700000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episode reward achieved: 28.0\n"
     ]
    }
   ],
   "source": [
    "test('BreakoutNoFrameskip-v4', 'models/breakout_dqn_best_4M.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
